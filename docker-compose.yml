services:
  llm_api:
    build:
      context: ./model_server
      args:
        MODEL_NAME: ${MODEL_NAME:-sshleifer/tiny-gpt2}
        MODEL_TYPE: ${MODEL_TYPE:-causal}
        MODEL_SERVER_PORT: ${MODEL_SERVER_PORT:-8000}
    ports:
      - "${MODEL_SERVER_PORT:-8000}:${MODEL_SERVER_PORT:-8000}"
    env_file:
      - ./model_server/.env

  chat_widget_app:
    build:
      context: ./node_server
    ports:
      - "5000:5000"
    environment:
      - MODEL_SERVER_HOST=llm_api
      - MODEL_SERVER_PORT=${MODEL_SERVER_PORT:-8000}
    depends_on:
      - llm_api
